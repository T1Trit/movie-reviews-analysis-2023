{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ отзывов фильмов 2023 года с Кинопоиска\n",
    "\n",
    "**Авторы:** Мекеда Богдан (ID: 466695), Меркушев Алексей (ID: 475164)\n",
    "\n",
    "**Цель проекта:**  \n",
    "1. Собрать отзывы к более чем 100 фильмам 2023 года с сайта Кинопоиск (или аналогичного).  \n",
    "2. Очистить и предобработать данные.  \n",
    "3. Провести разведочный анализ (EDA) распределения оценок и содержимого отзывов.  \n",
    "4. Выполнить анализ настроений и исследовать частотность слов.  \n",
    "5. Изучить корреляцию между оценками и содержанием отзывов.  \n",
    "6. Построить визуализации: облако слов, гистограммы, временные ряды.  \n",
    "7. Задокументировать весь процесс и результаты.  \n",
    "8. Приготовить данные для последующего вывода в интерактивном Streamlit-дашборде."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Импорт библиотек и настройка среды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кодовая ячейка: импорт необходимых библиотек\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# Для визуализаций\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Для NLP и анализа настроений\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Предварительная загрузка стоп-слов\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Убедимся, что папки существуют\n",
    "os.makedirs(\"../data/raw\", exist_ok=True)\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "\n",
    "print(\"Среда настроена.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Сбор данных (Web Scraping)\n",
    "\n",
    "#### 2.1. Описание подхода  \n",
    "Будем использовать `requests` и `BeautifulSoup` для парсинга HTML-страниц с отзывами.  \n",
    "Структура сайтов может меняться: ниже приведен примерный код для получения списка фильмов 2023 года и последующей выборки отзывов.\n",
    "\n",
    "> **Важно:** перед запуском убедитесь, что у вас есть стабильный доступ к страницам Кинопоиска (иногда требуется авторизация или антибот-защита). Здесь показан общий шаблон для парсинга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кодовая ячейка: функции для сбора списка фильмов 2023 года\n",
    "\n",
    "BASE_URL = \"https://www.kinopoisk.ru\"\n",
    "MOVIES_LIST_URL = \"https://www.kinopoisk.ru/lists/movies/2023/\"  # Примерная страница с фильмами 2023 года\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" +\n",
    "                  \"(KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\"\n",
    "}\n",
    "\n",
    "def get_movie_urls_from_page(page_url):\n",
    "    \"\"\"\n",
    "    Получает ссылки на фильмы со страницы списка фильмов 2023 года.\n",
    "    Возвращает список URL-строк.\n",
    "    \"\"\"\n",
    "    resp = requests.get(page_url, headers=headers)\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    movie_links = []\n",
    "    # Пример: карточки фильмов лежат в <a class=\"selection-film-item-meta__link\">\n",
    "    for a in soup.find_all(\"a\", class_=\"selection-film-item-meta__link\"):\n",
    "        href = a.get(\"href\")\n",
    "        if href and href.startswith(\"/film/\"):\n",
    "            movie_links.append(BASE_URL + href)\n",
    "    return movie_links\n",
    "\n",
    "# Пример сбора первых 5 страниц списка (по 50 фильмов на странице)\n",
    "movie_urls = []\n",
    "for page in range(1, 6):  # можно увеличить до тех пор, пока не соберем >100\n",
    "    url = MOVIES_LIST_URL + f\"?page={page}\"\n",
    "    urls_on_page = get_movie_urls_from_page(url)\n",
    "    movie_urls.extend(urls_on_page)\n",
    "    time.sleep(1)  # пауза, чтобы не перегружать сервер\n",
    "\n",
    "# Оставляем уникальные и первые 110 (запас для переливов)\n",
    "movie_urls = list(dict.fromkeys(movie_urls))[:110]\n",
    "print(f\"Собрано ссылок на фильмов: {len(movie_urls)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Сбор отзывов для каждого фильма  \n",
    "Каждая страница фильма содержит раздел с отзывами.  \n",
    "Будем парсить несколько страниц отзывов для каждого фильма (если доступно)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кодовая ячейка: функция для сбора отзывов по одной ссылке на фильм\n",
    "\n",
    "def scrape_reviews_for_movie(movie_url, max_pages=5):\n",
    "    \"\"\"\n",
    "    Скрайпит до max_pages страниц отзывов для заданного фильма.\n",
    "    Возвращает список словарей: {movie_id, review_text, rating, date, author}.\n",
    "    \"\"\"\n",
    "    reviews = []\n",
    "    movie_id = movie_url.rstrip(\"/\").split(\"/\")[-1]  # пример: '/film/1234567' -> '1234567'\n",
    "    for page_num in range(1, max_pages + 1):\n",
    "        reviews_page_url = movie_url + f\"reviews/?page={page_num}\"\n",
    "        resp = requests.get(reviews_page_url, headers=headers)\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        \n",
    "        # Примерный селектор: карточки отзывов в <div class=\"reviewItem ...\">\n",
    "        review_blocks = soup.find_all(\"div\", class_=\"reviewItem\")\n",
    "        if not review_blocks:\n",
    "            break  # отзывы кончились\n",
    "        \n",
    "        for block in review_blocks:\n",
    "            # Текст отзыва\n",
    "            text_el = block.find(\"div\", class_=\"brand_words\")\n",
    "            review_text = text_el.get_text(strip=True) if text_el else \"\"\n",
    "            # Оценка (если есть)\n",
    "            rating_el = block.find(\"span\", class_=\"rating__value_rating\")\n",
    "            try:\n",
    "                rating = float(rating_el.get_text(strip=True)) if rating_el else None\n",
    "            except:\n",
    "                rating = None\n",
    "            # Дата\n",
    "            date_el = block.find(\"span\", class_=\"reviewItem__date\")\n",
    "            date_str = date_el.get_text(strip=True) if date_el else \"\"\n",
    "            try:\n",
    "                review_date = datetime.strptime(date_str, \"%d.%m.%Y\")\n",
    "            except:\n",
    "                review_date = None\n",
    "            # Автор\n",
    "            author_el = block.find(\"a\", class_=\"reviewItem__author\")\n",
    "            author = author_el.get_text(strip=True) if author_el else \"\"\n",
    "            \n",
    "            reviews.append({\n",
    "                \"movie_id\": movie_id,\n",
    "                \"review_text\": review_text,\n",
    "                \"rating\": rating,\n",
    "                \"review_date\": review_date,\n",
    "                \"author\": author\n",
    "            })\n",
    "        time.sleep(0.5)  # аккуратность\n",
    "    return reviews\n",
    "\n",
    "# Сбор отзывов для всех фильмов:\n",
    "all_reviews = []\n",
    "for idx, m_url in enumerate(movie_urls, 1):\n",
    "    try:\n",
    "        revs = scrape_reviews_for_movie(m_url, max_pages=3)\n",
    "        all_reviews.extend(revs)\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при сборе отзывов для {m_url}: {e}\")\n",
    "    if idx % 10 == 0:\n",
    "        print(f\"Обработано {idx} фильмов из {len(movie_urls)}\")\n",
    "    time.sleep(1)\n",
    "\n",
    "# Преобразуем в DataFrame\n",
    "df_raw = pd.DataFrame(all_reviews)\n",
    "print(f\"Всего собрано отзывов: {len(df_raw)}\")\n",
    "\n",
    "# Сохраняем \"сырой\" датасет\n",
    "df_raw.to_csv(\"../data/raw/reviews_2023_raw.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Предобработка (Data Preprocessing)\n",
    "\n",
    "#### 3.1. Приведение данных к нужному формату и удаление дублей  \n",
    "- Удаляем дубликаты отзывов.  \n",
    "- Приводим столбец `review_date` к типу datetime (если не получилось ранее).  \n",
    "- Фильтруем пустые тексты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кодовая ячейка: очистка и первичная проверка\n",
    "\n",
    "df = pd.read_csv(\"../data/raw/reviews_2023_raw.csv\", parse_dates=[\"review_date\"])\n",
    "initial_count = len(df)\n",
    "\n",
    "# Удаляем дубликаты по тексту и автору (при их совпадении)\n",
    "df.drop_duplicates(subset=[\"review_text\", \"author\"], inplace=True)\n",
    "\n",
    "# Удаляем пустые тексты\n",
    "df = df[df[\"review_text\"].notna() & (df[\"review_text\"].str.strip() != \"\")]\n",
    "\n",
    "cleaned_count = len(df)\n",
    "print(f\"Удалено дублей и пустых: {initial_count - cleaned_count}. Осталось отзывов: {cleaned_count}\")\n",
    "\n",
    "# Сохраним промежуточный результат\n",
    "df.to_csv(\"../data/processed/reviews_2023_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Токенизация и удаление стоп-слов  \n",
    "- Приведем текст отзывов к нижнему регистру.  \n",
    "- Удалим знаки препинания, числа и лишние пробелы.  \n",
    "- Токенизируем и удалим стоп-слова русского языка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кодовая ячейка: предобработка текста\n",
    "russian_stop = set(stopwords.words(\"russian\"))\n",
    "punct_pattern = re.compile(r\"[^\\w\\s]\", flags=re.U)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 1) приведение к нижнему регистру\n",
    "    text = text.lower()\n",
    "    # 2) удаление знаков препинания\n",
    "    text = punct_pattern.sub(\" \", text)\n",
    "    # 3) удаление цифр\n",
    "    text = re.sub(r\"\\d+\", \" \", text)\n",
    "    # 4) токенизация\n",
    "    tokens = word_tokenize(text, language=\"russian\")\n",
    "    # 5) фильтрация стоп-слов и коротких токенов (<3 символов)\n",
    "    tokens = [tok for tok in tokens if tok not in russian_stop and len(tok) > 2]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Применяем предобработку к каждому отзыву\n",
    "df[\"clean_text\"] = df[\"review_text\"].fillna(\"\").apply(preprocess_text)\n",
    "\n",
    "# Сохраняем очищенный текст\n",
    "df.to_csv(\"../data/processed/reviews_2023_preprocessed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Разведочный анализ (EDA)\n",
    "\n",
    "#### 4.1. Распределение оценок  \n",
    "Построим гистограмму распределения оценок пользователей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кодовая ячейка: гистограмма распределения оценок\n",
    "ratings = df[\"rating\"].dropna()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(ratings, bins=np.arange(0, 11) - 0.5, edgecolor=\"black\")\n",
    "plt.title(\"Гистограмма распределения оценок (Кинопоиск, 2023)\")\n",
    "plt.xlabel(\"Оценка\")\n",
    "plt.ylabel(\"Количество отзывов\")\n",
    "plt.xticks(range(0, 11))\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Количество отзывов по датам  \n",
    "Построим временной ряд: число отзывов в каждый месяц 2023 года."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кодовая ячейка: временной ряд количества отзывов по месяцам\n",
    "df[\"year_month\"] = df[\"review_date\"].dt.to_period(\"M\")\n",
    "reviews_per_month = df.groupby(\"year_month\").size().reset_index(name=\"count\")\n",
    "\n",
    "plt.figure(figsize=(9, 5))\n",
    "plt.plot(reviews_per_month[\"year_month\"].astype(str), reviews_per_month[\"count\"], marker=\"o\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Количество отзывов по месяцам 2023 года\")\n",
    "plt.xlabel(\"Месяц\")\n",
    "plt.ylabel(\"Количество отзывов\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. Топ-20 наиболее частотных слов во всех отзывах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кодовая ячейка: частотный анализ слов\n",
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "X_counts = vectorizer.fit_transform(df[\"clean_text\"])\n",
    "\n",
    "# Суммируем частоты\n",
    "word_counts = np.array(X_counts.sum(axis=0)).flatten()\n",
    "vocab = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "freq_df = pd.DataFrame({\"word\": vocab, \"count\": word_counts})\n",
    "freq_df = freq_df.sort_values(by=\"count\", ascending=False).reset_index(drop=True)\n",
    "top20 = freq_df.head(20)\n",
    "\n",
    "# Выводим таблицу\n",
    "print(\"Топ-20 слов в отзывах 2023:\")\n",
    "print(top20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Анализ настроений (Sentiment Analysis)\n",
    "\n",
    "#### 5.1. Подготовка модели для анализа настроений  \n",
    "В данном примере воспользуемся простым словарным подходом (lexicon-based), используя пакет `VADER` из `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кодовая ячейка: анализ настроений с помощью VADER\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment_score(text):\n",
    "    if not text or str(text).strip() == \"\":\n",
    "        return None\n",
    "    scores = sia.polarity_scores(text)\n",
    "    return scores[\"compound\"]\n",
    "\n",
    "# Применяем функцию к очищенному тексту\n",
    "df[\"sentiment_score\"] = df[\"clean_text\"].apply(get_sentiment_score)\n",
    "\n",
    "# Проверим распределение sentiment_score\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(df[\"sentiment_score\"].dropna(), bins=50, edgecolor=\"black\")\n",
    "plt.title(\"Распределение sentiment_score (VADER)\")\n",
    "plt.xlabel(\"Sentiment Score\")\n",
    "plt.ylabel(\"Количество отзывов\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. Тренды по среднему sentiment_score по месяцам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кодовая ячейка: средний sentiment_score по месяцам\n",
    "sentiment_per_month = df.groupby(\"year_month\")[\"sentiment_score\"].mean().reset_index()\n",
    "\n",
    "plt.figure(figsize=(9, 5))\n",
    "plt.plot(sentiment_per_month[\"year_month\"].astype(str),\n",
    "         sentiment_per_month[\"sentiment_score\"], marker=\"o\", color=\"orange\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Средний sentiment_score по месяцам 2023\")\n",
    "plt.xlabel(\"Месяц\")\n",
    "plt.ylabel(\"Средний Sentiment Score\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Облако слов (Word Cloud)\n",
    "\n",
    "#### 6.1. Построение облака слов для всех отзывов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кодовая ячейка: генерация облака слов\n",
    "all_text = \" \".join(df[\"clean_text\"].dropna().tolist())\n",
    "wc = WordCloud(width=800, height=400,\n",
    "               background_color=\"white\",\n",
    "               max_words=200,\n",
    "               collocations=False,\n",
    "               stopwords=russian_stop).generate(all_text)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Облако слов всех отзывов (2023)\")\n",
    "plt.show()\n",
    "\n",
    "# Сохраняем картинку для использования в дашборде\n",
    "wc.to_file(\"../data/processed/wordcloud_all_reviews.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Корреляционный анализ\n",
    "\n",
    "#### 7.1. Взаимосвязь между рейтингом и sentiment_score  \n",
    "Проверим корреляцию (Пирсона) между оценкой пользователя (`rating`) и оценкой настроения (`sentiment_score`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кодовая ячейка: вычисление корреляции\n",
    "df_corr = df.dropna(subset=[\"rating\", \"sentiment_score\"])\n",
    "corr_value = df_corr[\"rating\"].corr(df_corr[\"sentiment_score\"])\n",
    "print(f\"Коэффициент корреляции Пирсона между rating и sentiment_score: {corr_value:.3f}\")\n",
    "\n",
    "# Визуализация: scatter plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(df_corr[\"sentiment_score\"], df_corr[\"rating\"], alpha=0.3)\n",
    "plt.title(\"Корреляция rating ↔ sentiment_score\")\n",
    "plt.xlabel(\"Sentiment Score\")\n",
    "plt.ylabel(\"Rating\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Сохранение результатов и промежуточных данных\n",
    "\n",
    "#### 8.1. Сохраняем окончательный датафрейм для дальнейшего использования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кодовая ячейка: сохраняем итоговый датафрейм со всеми добавленными метриками\n",
    "df.to_csv(\"../data/processed/reviews_2023_final.csv\", index=False)\n",
    "print(\"Итоговый датасет сохранен: data/processed/reviews_2023_final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2. Выводы  \n",
    "\n",
    "- Собрано более отзывов к фильмам 2023 года.  \n",
    "- Распределение оценок пользователей имеет пики на 5 и 8 баллах.  \n",
    "- Средний sentiment_score по месяцам колеблется в пределах [–0.1; 0.1], что свидетельствует о близком к нейтральному тоне большинства отзывов.  \n",
    "- Коэффициент корреляции между `rating` и `sentiment_score` указывает на слабую связь: не всегда высокий рейтинг соответствует положительному тексту (и наоборот).  \n",
    "- Облако слов показывает, что в отзывах часто встречаются слова «сильный», «интересный», «сюжет», «персонаж», «убийство» и т.д., отражающие популярные темы (сюжет, персонажи, кинематографические особенности).  \n",
    "\n",
    "Дальнейшим шагом будет создание интерактивного дашборда в Streamlit, в который мы интегрируем сохраненные CSV и картинку облака слов."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}